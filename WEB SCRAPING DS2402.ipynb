{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d44fa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name Rating  Year\n",
      "0                     Ship of Theseus      8  2012\n",
      "1                              Iruvar    8.4  1997\n",
      "2                     Kaagaz Ke Phool    7.8  1959\n",
      "3   Lagaan: Once Upon a Time in India    8.1  2001\n",
      "4                     Pather Panchali    8.2  1955\n",
      "..                                ...    ...   ...\n",
      "95                        Apur Sansar    8.4  1959\n",
      "96                        Kanchivaram    8.2  2008\n",
      "97                    Monsoon Wedding    7.3  2001\n",
      "98                              Black    8.1  2005\n",
      "99                            Deewaar      8  1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "movie_container = soup.find('div', class_='lister-list')\n",
    "\n",
    "\n",
    "movie_names = []\n",
    "movie_ratings = []\n",
    "movie_years = []\n",
    "\n",
    "\n",
    "for movie in movie_container.find_all('div', class_='lister-item-content'):\n",
    "    name = movie.find('a').text.strip()\n",
    "    movie_names.append(name)\n",
    "    rating = movie.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "    movie_ratings.append(rating)\n",
    "    year = movie.find('span', class_='lister-item-year').text.strip('()')\n",
    "    movie_years.append(year)\n",
    "movie_data = {\n",
    "    'Name': movie_names,\n",
    "    'Rating': movie_ratings,\n",
    "    'Year': movie_years\n",
    "}\n",
    "df = pd.DataFrame(movie_data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ab880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 1:\n",
      "Name: Women's Pink & White Camo Printed Oversized Short Top\n",
      "Price: \n",
      "Image URL: https://images.bewakoof.com/t640/women-s-pink-white-camo-printed-oversized-t-shirt-580369-1686301190-1.jpg\n",
      "\n",
      "Product 2:\n",
      "Name: Women's White Bored Typography Oversized Short Top\n",
      "Price: \n",
      "Image URL: https://images.bewakoof.com/t640/women-white-printed-top-26-582038-1689082446-1.jpg\n",
      "\n",
      "Product 3:\n",
      "Name: Women's White & Black All Over Mickey Printed Oversized Short Top\n",
      "Price: \n",
      "Image URL: https://images.bewakoof.com/t640/women-s-white-oversized-t-shirt-584434-1684230405-1.jpg\n",
      "\n",
      "Product 4:\n",
      "Name: Women's Black All Over Printed Oversized Short Top\n",
      "Price: \n",
      "Image URL: https://images.bewakoof.com/t640/women-aop-oversize-t-shirt-3-580366-1682421809-1.JPG\n",
      "\n",
      "Product 5:\n",
      "Name: Women's White All Over Tweety Printed Oversized Short Top\n",
      "Price: \n",
      "Image URL: https://images.bewakoof.com/t640/women-aop-oversize-t-shirt-2-580367-1685446636-1.jpg\n",
      "\n",
      "Product 6:\n",
      "Name: Women's White Peeking Army Graphic Printed Boyfriend T-shirt\n",
      "Price: \n",
      "Image URL: https://images.bewakoof.com/t640/women-s-white-peeking-army-graphic-printed-boyfriend-t-shirt-519415-1680593017-1.jpg\n",
      "\n",
      "Product 7:\n",
      "Name: Women's White MOTD Panda Graphic Printed T-shirt\n",
      "Price: \n",
      "Image URL: https://images.bewakoof.com/t640/women-s-white-motd-panda-t-shirt-479771-1655815714-1.jpg\n",
      "\n",
      "Product 8:\n",
      "Name: Women's Black No We in Food Graphic Printed Boyfriend T-shirt\n",
      "Price: \n",
      "Image URL: https://images.bewakoof.com/t640/women-s-black-no-we-in-food-graphic-printed-boyfriend-t-shirt-483824-1656155113-1.jpg\n",
      "\n",
      "Product 9:\n",
      "Name: Women's Black Fragile Graphic Printed Boyfriend T-shirt\n",
      "Price: \n",
      "Image URL: https://images.bewakoof.com/t640/fragile-boyfriend-t-shirt-516553-1657179813-1.jpg\n",
      "\n",
      "Product 10:\n",
      "Name: Men's Black Batman Riddle (Bml) Oversized T-shirt\n",
      "Price: \n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-batman-riddle-bml-oversized-t-shirt-479765-1709216751-1.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "product_list = soup.find_all('div', class_='productCardBox')\n",
    "\n",
    "for index, product in enumerate(product_list[:10], 1):\n",
    "    product_name = product.find('h2', class_='clr-shade4 h3-p-name undefined false').text.strip()\n",
    "    price = product.find('span', class_='sellingFastBox').text.strip()\n",
    "    image_url = product.find('img', class_='productImgTag')['src']\n",
    "    \n",
    "    print(f\"Product {index}:\")\n",
    "    print(f\"Name: {product_name}\")\n",
    "    print(f\"Price: {price}\")\n",
    "    print(f\"Image URL: {image_url}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4033b6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House details for Indira Nagar:\n",
      "Title: []\n",
      "Location: []\n",
      "Area: []\n",
      "EMI: []\n",
      "Price: []\n",
      "House details for Jayanagar:\n",
      "Title: []\n",
      "Location: []\n",
      "Area: []\n",
      "EMI: []\n",
      "Price: []\n",
      "House details for Rajaji Nagar:\n",
      "Title: []\n",
      "Location: []\n",
      "Area: []\n",
      "EMI: []\n",
      "Price: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "localities = ['Indira Nagar', 'Jayanagar', 'Rajaji Nagar']\n",
    "\n",
    "for locality in localities:\n",
    "    \n",
    "    url = f'https://www.nobroker.in/{locality}/for-sale'\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "   \n",
    "    house_containers = soup.find_all('div', class_='css-tlx5bp')\n",
    "\n",
    "\n",
    "    titles = []\n",
    "    locations = []\n",
    "    areas = []\n",
    "    emis = []\n",
    "    prices = []\n",
    "\n",
    "    for house_container in house_containers:\n",
    "        title = house_container.find('h2', class_='heading-6 flex items-center font-semi-bold m-0').text.strip()\n",
    "        titles.append(title)\n",
    "\n",
    "    \n",
    "        location = house_container.find('div', class_='mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0.1p po:max-w-95\"').text.strip()\n",
    "        location = location.split(',')[0].strip()\n",
    "        locations.append(location)\n",
    "\n",
    "        \n",
    "        area = house_container.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "        area = re.findall(r'(\\d+) Sqft', area)\n",
    "        if area:\n",
    "            area = area[0]\n",
    "            areas.append(area)\n",
    "        else:\n",
    "            areas.append('')\n",
    "\n",
    " \n",
    "        emi = house_container.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "        emi = re.findall(r'(\\d+) EMI', emi)\n",
    "        if emi:\n",
    "            emi = emi[0]\n",
    "            emis.append(emi)\n",
    "        else:\n",
    "            emis.append('')\n",
    "\n",
    "        price = house_container.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "        price = re.findall(r'(\\d+) Lakh', price)\n",
    "        if price:\n",
    "            price = price[0]\n",
    "            prices.append(price)\n",
    "        else:\n",
    "            prices.append('')\n",
    "\n",
    "    print(f'House details for {locality}:')\n",
    "    print('Title:', titles)\n",
    "    print('Location:', locations)\n",
    "    print('Area:', areas)\n",
    "    print('EMI:', emis)\n",
    "    print('Price:', prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eccb2d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headings: []\n",
      "Dates: []\n",
      "Links: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "headings = []\n",
    "dates = []\n",
    "links = []\n",
    "\n",
    "for article in soup.find_all(\"article\"):\n",
    "    heading = article.find(\"div class=SecondaryCard-imageContainer\").text\n",
    "    date = article.find(\"time class=LatestNews-timestamp\")[\"datetime\"]\n",
    "    link = article.find(\"a\")[\"href\"]\n",
    "\n",
    "    headings.append(heading)\n",
    "    dates.append(date)\n",
    "    links.append(link)\n",
    "\n",
    "print(\"Headings:\", headings)\n",
    "print(\"Dates:\", dates)\n",
    "print(\"Links:\", links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75a4adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "articles = soup.find_all('div', class_='card-footer')\n",
    "\n",
    "paper_titles = []\n",
    "dates = []\n",
    "authors = []\n",
    "\n",
    "for article in articles:\n",
    "\n",
    "    paper_title = article.find('h1').text.strip()\n",
    "    paper_titles.append(paper_title)\n",
    "\n",
    "    date = article.find('div', class_='Text--ellipsis Text--secondary').text.strip()\n",
    "    dates.append(date)\n",
    "\n",
    "    author = article.find('div', class_='col-auto').text.strip()\n",
    "    authors.append(author)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d56366bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper Title: []\n",
      "Date: []\n",
      "Author: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Paper Title:\", paper_titles)\n",
    "print(\"Date:\", dates)\n",
    "print(\"Author:\", authors)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "043c8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.patreon.com/coreyms\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "posts = soup.find_all('div', class_='post-container')\n",
    "\n",
    "headings = []\n",
    "dates = []\n",
    "contents = []\n",
    "likes = []\n",
    "\n",
    "for post in posts:\n",
    "\n",
    "    heading = post.find('div', class_='sc-XxNYO bJYvKa').text.strip()\n",
    "    headings.append(heading)\n",
    "\n",
    "\n",
    "    date = post.find('time')['datetime']\n",
    "    dates.append(date)\n",
    "\n",
    " \n",
    "    content = post.find('div', class_='sc-jvvksu fTaUtt')\n",
    "    if content:\n",
    "        content = content.text.strip()\n",
    "    else:\n",
    "        content = ''\n",
    "    contents.append(content)\n",
    "\n",
    "    youtube_link = post.find('div', class_='patreon-media').find('a')['href']\n",
    "    if 'youtube.com' in youtube_link:\n",
    "        \n",
    "        youtube_response = requests.get(youtube_link)\n",
    "      \n",
    "        youtube_soup = BeautifulSoup(youtube_response.content, 'html.parser')\n",
    "       \n",
    "        likes_element = youtube_soup.find('span', class_='sc-bqiRlB etUZPh')\n",
    "        likes_text = likes_element.text if likes_element else 'Likes not available'\n",
    "        likes.append(likes_text)\n",
    "    else:\n",
    "        likes.append('Video not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d897323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading: []\n",
      "Date: []\n",
      "Content: []\n",
      "Likes: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Heading:\", headings)\n",
    "print(\"Date:\", dates)\n",
    "print(\"Content:\", contents)\n",
    "print(\"Likes:\", likes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c69bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
