{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbd920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def search_amazon_product(product):\n",
    "    driver = webdriver.Chrome()  \n",
    "    driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "    search_bar = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(product)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    products = driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "    product_titles = [product.text for product in products]\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return product_titles\n",
    "\n",
    "user_input = input(\"Enter the product to search for on Amazon India: \")\n",
    "search_results = search_amazon_product(user_input)\n",
    "\n",
    "if search_results:\n",
    "    print(\"Search Results:\")\n",
    "    for i, result in enumerate(search_results, 1):\n",
    "        print(f\"{i}. {result}\")\n",
    "else:\n",
    "    print(\"No products found for the given input.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1551ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "def scrape_product_details(product, num_pages):\n",
    "    driver = webdriver.Chrome()  \n",
    "    driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "    search_bar = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(product)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for page in range(num_pages):\n",
    "        try:\n",
    "           \n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//span[@class='a-size-medium a-color-base a-text-normal']\")))\n",
    "        \n",
    "            products = driver.find_elements_by_xpath(\"//div[@data-component-type='s-search-result']\")\n",
    "            for product in products:\n",
    "                try:\n",
    "                    brand_name = product.find_element_by_xpath(\".//span[@class='a-size-base-plus a-color-base']\").text\n",
    "                    product_name = product.find_element_by_xpath(\".//span[@class='a-size-medium a-color-base a-text-normal']\").text\n",
    "                    price = product.find_element_by_xpath(\".//span[@class='a-price-whole']\").text\n",
    "                    return_exchange = product.find_element_by_xpath(\".//span[contains(text(),'Exchange') or contains(text(),'Returns')]//following-sibling::span\").text\n",
    "                    expected_delivery = product.find_element_by_xpath(\".//span[contains(text(),'FREE Delivery')]//following-sibling::span\").text\n",
    "                    availability = product.find_element_by_xpath(\".//span[contains(text(),'In stock')]//ancestor::div[@class='a-section a-spacing-none a-spacing-top-micro']\").text\n",
    "                    product_url = product.find_element_by_xpath(\".//a[@class='a-link-normal a-text-normal']\").get_attribute('href')\n",
    "\n",
    "                    product_details.append({\n",
    "                        \"Brand Name\": brand_name,\n",
    "                        \"Name of the Product\": product_name,\n",
    "                        \"Price\": price,\n",
    "                        \"Return/Exchange\": return_exchange,\n",
    "                        \"Expected Delivery\": expected_delivery,\n",
    "                        \"Availability\": availability,\n",
    "                        \"Product URL\": product_url\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(\"Error scraping product details:\", e)\n",
    "                    continue\n",
    "          \n",
    "            next_button = driver.find_element_by_xpath(\"//li[@class='a-last']//a\")\n",
    "            if \"Next\" in next_button.text:\n",
    "                next_button.click()\n",
    "                time.sleep(2)  \n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(\"Error scraping page:\", e)\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return pd.DataFrame(product_details)\n",
    "\n",
    "user_input = input(\"Enter the product to search for on Amazon India: \")\n",
    "num_pages = 3  \n",
    "product_details_df = scrape_product_details(user_input, num_pages)\n",
    "\n",
    "product_details_df.fillna('-', inplace=True)\n",
    "\n",
    "product_details_df.to_csv('amazon_product_details.csv', index=False)\n",
    "\n",
    "print(\"Scraping and saving completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ee4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def scrape_images(keyword, num_images):\n",
    "    driver = webdriver.Chrome() \n",
    "    driver.get(\"https://www.google.com/imghp?hl=en\")\n",
    "\n",
    "    search_bar = driver.find_element_by_xpath(\"//input[@class='gLFyf gsfi']\")\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    for _ in range(3):  \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    image_elements = driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\")\n",
    "    image_urls = []\n",
    "    for img in image_elements[:num_images]:\n",
    "        try:\n",
    "            img.click()\n",
    "            time.sleep(1)\n",
    "            actual_image = driver.find_element_by_css_selector('img.n3VNCb')\n",
    "            if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                image_urls.append(actual_image.get_attribute('src'))\n",
    "        except Exception as e:\n",
    "            print(\"Error while scraping image URL:\", e)\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_images_per_keyword = 10\n",
    "\n",
    "for keyword in keywords:\n",
    "    image_urls = scrape_images(keyword, num_images_per_keyword)\n",
    "    save_directory = f'images/{keyword}'\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    for i, image_url in enumerate(image_urls):\n",
    "        try:\n",
    "            image_content = requests.get(image_url).content\n",
    "            with open(os.path.join(save_directory, f'{keyword}_{i+1}.jpg'), 'wb') as f:\n",
    "                f.write(image_content)\n",
    "        except Exception as e:\n",
    "            print(\"Error while saving image:\", e)\n",
    "\n",
    "print(\"Scraping and saving completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def scrape_smartphones(keyword):\n",
    "    driver = webdriver.Chrome()  \n",
    "    driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "    search_bar = driver.find_element_by_xpath(\"//input[@title='Search for products, brands and more']\")\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='_4rR01T']\")))\n",
    "\n",
    "    products = driver.find_elements_by_xpath(\"//div[@class='_4rR01T']\")\n",
    "    smartphone_details = []\n",
    "    for product in products:\n",
    "        try:\n",
    "            brand_name = product.find_element_by_xpath(\".//div[@class='_2WkVRV']\").text\n",
    "            smartphone_name = product.find_element_by_xpath(\".//a[@class='s1Q9rs']\").text\n",
    "            color = product.find_element_by_xpath(\".//a[@class='IRpwTa']\").get_attribute('title')\n",
    "            ram = product.find_element_by_xpath(\".//li[contains(text(),'RAM')]\").text\n",
    "            storage = product.find_element_by_xpath(\".//li[contains(text(),'ROM')]\").text\n",
    "            primary_camera = product.find_element_by_xpath(\".//li[contains(text(),'Primary')]\").text\n",
    "            secondary_camera = product.find_element_by_xpath(\".//li[contains(text(),'Secondary')]\").text\n",
    "            display_size = product.find_element_by_xpath(\".//li[contains(text(),'Display')]\").text\n",
    "            battery_capacity = product.find_element_by_xpath(\".//li[contains(text(),'Battery')]\").text\n",
    "            price = product.find_element_by_xpath(\".//div[@class='_30jeq3']\").text\n",
    "            product_url = product.find_element_by_xpath(\".//a[@class='s1Q9rs']\").get_attribute('href')\n",
    "\n",
    "            smartphone_details.append({\n",
    "                \"Brand Name\": brand_name,\n",
    "                \"Smartphone Name\": smartphone_name,\n",
    "                \"Colour\": color,\n",
    "                \"RAM\": ram,\n",
    "                \"Storage(ROM)\": storage,\n",
    "                \"Primary Camera\": primary_camera,\n",
    "                \"Secondary Camera\": secondary_camera,\n",
    "                \"Display Size\": display_size,\n",
    "                \"Battery Capacity\": battery_capacity,\n",
    "                \"Price\": price,\n",
    "                \"Product URL\": product_url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error while scraping product details:\", e)\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return pd.DataFrame(smartphone_details)\n",
    "\n",
    "user_input = input(\"Enter the smartphone to search for on Flipkart: \")\n",
    "smartphone_details_df = scrape_smartphones(user_input)\n",
    "\n",
    "smartphone_details_df.fillna('-', inplace=True)\n",
    "\n",
    "smartphone_details_df.to_csv('flipkart_smartphone_details.csv', index=False)\n",
    "\n",
    "print(\"Scraping and saving completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b265741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "driver = selenium.webdriver.Chrome()\n",
    "\n",
    "city = \"New York City\"\n",
    "search_parameters = {\"route\": city}\n",
    "\n",
    "driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "search_box = driver.find_element_by_name(\"q\")\n",
    "search_box.send_keys(city)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".section-result\")))\n",
    "\n",
    "results = driver.find_elements_by_css_selector(\".section-result\")\n",
    "coordinates = []\n",
    "for result in results:\n",
    "    latitude = result.find_element_by_css_selector(\".latitude\").text\n",
    "    longitude = result.find_element_by_css_selector(\".longitude\").text\n",
    "    coordinates.append((latitude, longitude))\n",
    "\n",
    "print(coordinates)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebcd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading active sticky-footer')\n",
    "\n",
    "        laptop_details = []\n",
    "        for laptop in laptops:\n",
    "            try:\n",
    "                laptop_name = laptop.find('div', class_='heading-wraper').text.strip()\n",
    "                specs = laptop.find_all('div', class_='specs-wraper')[0].find_all('div', class_='value')\n",
    "                processor = specs[0].text.strip()\n",
    "                memory = specs[1].text.strip()\n",
    "                os = specs[2].text.strip()\n",
    "                display = specs[3].text.strip()\n",
    "                dimensions = specs[4].text.strip()\n",
    "                weight = specs[5].text.strip()\n",
    "                gpu = specs[6].text.strip()\n",
    "                price = laptop.find('div', class_='smprice').text.strip()\n",
    "\n",
    "                laptop_details.append({\n",
    "                    \"Laptop Name\": laptop_name,\n",
    "                    \"Processor\": processor,\n",
    "                    \"Memory\": memory,\n",
    "                    \"Operating System\": os,\n",
    "                    \"Display\": display,\n",
    "                    \"Dimensions\": dimensions,\n",
    "                    \"Weight\": weight,\n",
    "                    \"Graphics Processor\": gpu,\n",
    "                    \"Price\": price\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"Error scraping laptop details:\", e)\n",
    "                continue\n",
    "\n",
    "        return pd.DataFrame(laptop_details)\n",
    "    else:\n",
    "        print(\"Failed to fetch page\")\n",
    "        return None\n",
    "\n",
    "gaming_laptops_df = scrape_gaming_laptops()\n",
    "\n",
    "if gaming_laptops_df is not None:\n",
    "    gaming_laptops_df.to_csv('gaming_laptops_digit.csv', index=False)\n",
    "    print(\"Scraping and saving completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.forbes.com/billionaires/?list=worlds-billionaires\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "billionaires = soup.find_all(\"tr\", class_=\"profile-list__item\")\n",
    "for billionaire in billionaires:\n",
    "    rank = billionaire.find(\"td\", class_=\"profile-list__item-rank\").text.strip()\n",
    "    name = billionaire.find(\"td\", class_=\"profile-list__item-name\").text.strip()\n",
    "    net_worth = billionaire.find(\"td\", class_=\"profile-list__item-money\").text.strip()\n",
    "    age = billionaire.find(\"td\", class_=\"profile-list__item-age\")\n",
    "    if age:\n",
    "        age = age.text.strip()\n",
    "    else:\n",
    "        age = \"N/A\"\n",
    "    citizenship = billionaire.find(\"td\", class_=\"profile-list__item-citizenship\").text.strip()\n",
    "    source = billionaire.find(\"td\", class_=\"profile-list__item-source\")\n",
    "    if source:\n",
    "        source = source.text.strip()\n",
    "    else:\n",
    "        source = \"N/A\"\n",
    "    industry = billionaire.find(\"td\", class_=\"profile-list__item-industry\").text.strip()\n",
    "    print(f\"Rank: {rank}\")\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Net worth: {net_worth}\")\n",
    "    print(f\"Age: {age}\")\n",
    "    print(f\"Citizenship: {citizenship}\")\n",
    "    print(f\"Source: {source}\")\n",
    "    print(f\"Industry: {industry}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbaead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "API_KEY = \"YOUR_API_KEY\"  \n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "def get_video_comments(video_id, max_results=500):\n",
    "    try:\n",
    "      \n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "\n",
    "        while len(comments) < max_results:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                maxResults=min(max_results - len(comments), 100),\n",
    "                pageToken=next_page_token\n",
    "            ).execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_text = comment['textDisplay']\n",
    "                comment_like_count = comment['likeCount']\n",
    "                comment_published_time = comment['publishedAt']\n",
    "                comment_published_time = datetime.strptime(comment_published_time, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                comments.append({\n",
    "                    \"Comment\": comment_text,\n",
    "                    \"Like Count\": comment_like_count,\n",
    "                    \"Published Time\": comment_published_time\n",
    "                })\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                next_page_token = response['nextPageToken']\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return comments\n",
    "    except HttpError as e:\n",
    "        print(f\"An HTTP error {e.resp.status} occurred: {e.content}\")\n",
    "        return []\n",
    "\n",
    "def save_comments_to_csv(video_id, comments):\n",
    "    filename = f\"comments_{video_id}.csv\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Comment', 'Like Count', 'Published Time']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for comment in comments:\n",
    "            writer.writerow(comment)\n",
    "\n",
    "    print(f\"Comments saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = input(\"Enter the YouTube video ID: \")\n",
    "    comments = get_video_comments(video_id)\n",
    "    if comments:\n",
    "        save_comments_to_csv(video_id, comments)\n",
    "    else:\n",
    "        print(\"No comments found for the video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a63f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostels(location):\n",
    "    url = f\"https://www.hostelworld.com/findabed.php/ChosenCity.London/ChosenCountry.England\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        hostel_listings = soup.find_all(\"div\", class_=\"hwta-property\")\n",
    "        \n",
    "        hostels_data = []\n",
    "        for hostel in hostel_listings:\n",
    "            hostel_name = hostel.find(\"h2\", class_=\"title\").text.strip()\n",
    "            distance_from_city_centre = hostel.find(\"span\", class_=\"hwta-property-cta distance\").text.strip()\n",
    "            ratings = hostel.find(\"div\", class_=\"score orange\").text.strip()\n",
    "            total_reviews = hostel.find(\"div\", class_=\"reviews\").text.strip()\n",
    "            overall_reviews = hostel.find(\"div\", class_=\"keyword\").text.strip()\n",
    "            privates_from_price = hostel.find(\"div\", class_=\"prices\").find(\"div\", class_=\"private-prices\").text.strip()\n",
    "            dorms_from_price = hostel.find(\"div\", class_=\"prices\").find(\"div\", class_=\"dorm-prices\").text.strip()\n",
    "            facilities = [facility.text.strip() for facility in hostel.find_all(\"div\", class_=\"facilities\")]\n",
    "            property_description = hostel.find(\"div\", class_=\"description\").text.strip()\n",
    "            \n",
    "            hostels_data.append({\n",
    "                \"Hostel Name\": hostel_name,\n",
    "                \"Distance from City Centre\": distance_from_city_centre,\n",
    "                \"Ratings\": ratings,\n",
    "                \"Total Reviews\": total_reviews,\n",
    "                \"Overall Reviews\": overall_reviews,\n",
    "                \"Privates From Price\": privates_from_price,\n",
    "                \"Dorms From Price\": dorms_from_price,\n",
    "                \"Facilities\": facilities,\n",
    "                \"Property Description\": property_description\n",
    "            })\n",
    "        \n",
    "        return hostels_data\n",
    "    else:\n",
    "        print(\"Failed to fetch page\")\n",
    "        return None\n",
    "\n",
    "london_hostels_data = scrape_hostels(\"London\")\n",
    "\n",
    "if london_hostels_data:\n",
    "    print(\"Number of hostels found:\", len(london_hostels_data))\n",
    "    print(\"Example Hostel Data:\")\n",
    "    print(london_hostels_data[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
